{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf5fa8c",
   "metadata": {},
   "source": [
    "# **Project 17**: Post-Event Damage Assessment Using News Article Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a56715",
   "metadata": {},
   "source": [
    "## Group Members:\n",
    "- Walid Nouicer (wnouicer24@student.oulu.fi)\n",
    "- Uswah Batool (uswah.batool@student.oulu.fi)\n",
    "- Piero Campos Villagaray (pcamposv25@student.oulu.fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda42a1b",
   "metadata": {},
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c1583",
   "metadata": {},
   "source": [
    "### Task 1: Event Selection and Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b912e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_1 import get_urls, get_articles, articles2csv, append_article_to_csv\n",
    "\n",
    "# get_urls() can be used to extract urls from news sources\n",
    "# if no news sources are passed in function parameters, the function uses a default list\n",
    "# this process can take a very long time, for the sake of the demo we will use a preset example\n",
    "\n",
    "# urls = get_urls()\n",
    "\n",
    "# the natural disaster we will be using as an example is Hurricane Melissa, which is an active\n",
    "# Atlantic hurricane currently accelerating northeastward away from the Bahamas and toward Bermuda.\n",
    "# get_articles() filters and scrapes article data from urls based on qwery words passed into the function\n",
    "# we can save articles to a csv file during he function's runtime by setting live_save=True, default value is False\n",
    "# default path for the csv fileis \"data/articles.csv\"\n",
    "query = [\"hurricane\", \"melissa\"]\n",
    "\n",
    "# uncomment this for a quick example\n",
    "example_urls = [\n",
    "    \"https://www.aljazeera.com/gallery/2025/10/30/hurricane-melissa-leaves-trail-of-destruction-across-northern-caribbean\",\n",
    "    \"https://apnews.com/article/hurricane-melissa-jamaica-cuba-landslide-rain-flood-d7d120b8443b1630d12c77e0a3fe25b0\",\n",
    "]\n",
    "articles = get_articles(example_urls, query, limit=200, live_save=False)\n",
    "\n",
    "# uncomment this for the real scraping process (WARNING: it can run for a long time)\n",
    "# articles = get_articles(urls, query, limit=200, live_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dccd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying saved data n csv\n",
    "\n",
    "for article in articles:\n",
    "    print(\n",
    "        f\"\\nTitle: {article[0]}\\ndate: {article[1]}\\nSource: {article[2]}\\nText: {article[3][:200]}...\\n\"\n",
    "    )\n",
    "    print(\"___________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f17d4",
   "metadata": {},
   "source": [
    "### Task 2: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb619ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_2 import load_file, clean_text\n",
    "\n",
    "# load_file() automatically loads data from \"data/articles.csv\", a different path can be passed in function params\n",
    "data = load_file()\n",
    "\n",
    "# clean_text() performs lemmatization, and removes stopwords, punctuation, symbols, and converts text to lowecase\n",
    "# cleaned_data = clean_text(data)\n",
    "\n",
    "# articles2csv() can be used to save the processed data\n",
    "# articles2csv(\n",
    "#     articles=cleaned_data,\n",
    "#     path=\"data/cleaned_data.csv\",\n",
    "#     fields=[\"title\", \"date\", \"source\", \"article_text\", \"clean_text\"],\n",
    "# )\n",
    "\n",
    "# the clean_text column is saved as a long string for convenience\n",
    "print(f\"{load_file(path=\"data/cleaned_data.csv\")[1][4][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59a4d6",
   "metadata": {},
   "source": [
    "### Task 3: Language Filtering and Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ab7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from task_3 import filter_english_articles_with_descriptive_stats\n",
    "\n",
    "# filter_english_articles_with_descriptive_stats() filters non-english and duplicate \n",
    "# articles, then prints a quality report\n",
    "\n",
    "data = pd.read_csv(\"data/cleaned_data.csv\")\n",
    "filter_english_articles_with_descriptive_stats(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c842739",
   "metadata": {},
   "source": [
    "### Task 4: Keyword Extraction and Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe24771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_4 import plot_and_save, keyword_analysis\n",
    "\n",
    "# keyword_analysis() extract top keywords using both CountVectorizer and TF-IDF,\n",
    "# visualize results in bar charts and word clouds\n",
    "data = pd.read_csv(\"data/filtered_articles.csv\")\n",
    "keyword_analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699404f",
   "metadata": {},
   "source": [
    "### Task 5: Corpus Statistical Analysis and Zipf’s Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_5 import zipf_analysis\n",
    "\n",
    "# zipf_analysis() performs Zipf’s Law analysis and visualize rank-frequency relationship\n",
    "data = pd.read_csv(\"data/filtered_articles.csv\")\n",
    "zipf_analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671d4e8",
   "metadata": {},
   "source": [
    "### Task 6: Lexical and Readability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_6 import analyze_article_metrics\n",
    "\n",
    "df = pd.read_csv(r\"data/articles.csv\") \n",
    "print(f\"Total articles loaded: {len(df)}\")\n",
    "\n",
    "# analyze_article_metrics() calculates lexical metrics and returns results\n",
    "lexical_results = []\n",
    "for idx, row in df.iterrows():\n",
    "    result = analyze_article_metrics(row)\n",
    "    if result is not None:\n",
    "        lexical_results.append(result)\n",
    "\n",
    "lexical_df = pd.DataFrame(lexical_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LEXICAL METRICS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(lexical_df.describe())  ## Summary Table\n",
    "\n",
    "print(\"\\nSENTENCE COUNT:\")\n",
    "print(f\"  Min sentences: {lexical_df['sentence_count'].min()}\")\n",
    "print(f\"  Max sentences: {lexical_df['sentence_count'].max()}\")\n",
    "print(f\"  Mean sentences: {lexical_df['sentence_count'].mean():.1f}\")\n",
    "\n",
    "lexical_df.to_csv(r\"data/Lexical_Analysis.csv\", index=False)\n",
    "print(f\"\\n Results saved to 'Lexical_Analysis.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e45819",
   "metadata": {},
   "source": [
    "### Task 7: Sentiment and Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_7 import comprehensive_analysis\n",
    "\n",
    "\n",
    "\n",
    "# Load DataFrame\n",
    "# df = pd.read_csv(r\".\\data\\cleaned_data.csv\")\n",
    "df = pd.read_csv(r\"data/cleaned_data.csv\")\n",
    "print(f\"\\nLoaded {len(df)} articles\")\n",
    "\n",
    "\n",
    "# Execute the analysis\n",
    "\n",
    "sentiment_df = comprehensive_analysis(df)\n",
    "\n",
    "# Saved\n",
    "# sentiment_df.to_csv(r\".\\data\\sentiment_emotion_analysis.csv\", index=False)\n",
    "sentiment_df.to_csv(r\"data/sentiment_emotion_analysis.csv\", index=False)\n",
    "print(f\"Results saved to 'sentiment_emotion_analysis.csv'\")\n",
    "\n",
    "# Statistics\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sentiment\n",
    "print(\"\\nSENTIMENT ANALYSIS (VADER):\")\n",
    "sentiment_cols = [\"vader_compound\", \"vader_positive\", \"vader_negative\", \"vader_neutral\"]\n",
    "print(sentiment_df[sentiment_cols].describe())\n",
    "\n",
    "# Emotions\n",
    "print(\"\\nEMOTION DETECTION (Hugging Face Transformer):\")\n",
    "emotion_cols = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "print(sentiment_df[emotion_cols].describe())\n",
    "\n",
    "# Important things\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Average sentiment\n",
    "avg_sentiment = sentiment_df[\"vader_compound\"].mean()\n",
    "print(f\"\\n Average Sentiment (VADER): {avg_sentiment:.3f}\")\n",
    "if avg_sentiment < -0.3:\n",
    "    print(\"   - Highly negative \")\n",
    "elif avg_sentiment < -0.1:\n",
    "    print(\"   - Moderately negative\")\n",
    "else:\n",
    "    print(\"   - Neutral/Mixed coverage\")\n",
    "\n",
    "# Top 3 emotions\n",
    "print(\"\\nTop 3 Emotions (by average intensity):\")\n",
    "avg_emotions = sentiment_df[emotion_cols].mean().sort_values(ascending=False)\n",
    "for i, (emotion, score) in enumerate(avg_emotions.head(3).items(), 1):\n",
    "    print(f\"   {i}. {emotion.capitalize():10s}: {score:.3f}\")\n",
    "\n",
    "# Fear/Sadness analysis\n",
    "fear_avg = sentiment_df[\"fear\"].mean()\n",
    "sadness_avg = sentiment_df[\"sadness\"].mean()\n",
    "fs_ratio = sentiment_df[\"fear_sadness_ratio\"].mean()\n",
    "\n",
    "print(f\"\\nFear vs Sadness Analysis:\")\n",
    "print(f\"   Fear average:    {fear_avg:.3f}\")\n",
    "print(f\"   Sadness average: {sadness_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb8cb9",
   "metadata": {},
   "source": [
    "### Task 8: Named Entity and Quantitative Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_8 import summary, plot_entity_frequency, aggregate_text, affected_regions, affected_regions_summary\n",
    "\n",
    "# damage_summary() calculate the number of instances in a dataset where and entity\n",
    "# is labeled as MONEY or CARDINAL. for the latter case, filter out\n",
    "# sentences describing distance or speed measurements.\n",
    "\n",
    "aggr_text = aggregate_text(data=load_file()[1:])\n",
    "plot_entity_frequency(aggr_text)\n",
    "print(\"_________________________________________________________________\\n\")\n",
    "damage_summary = summary(aggr_text,summary_size=7)\n",
    "print(\"Summary of the damages:\\n\")\n",
    "for s in damage_summary:\n",
    "    print(f\"- {s}\")\n",
    "print(\"_________________________________________________________________\\n\")\n",
    "regions = affected_regions(aggr_text)\n",
    "affected_regions_summary(regions, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f3fd9",
   "metadata": {},
   "source": [
    "### Task 9: Event Impact Scoring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_9 import impact2csv\n",
    "from colored_text import bcolors\n",
    "data = load_file(\"data/cleaned_data.csv\")\n",
    "\n",
    "# impact2csv() takes csv data and computes sentiment polarity, emotion intensity, damage-related keywords frequency, and impact score\n",
    "# then saves the result into a csv file and returns the generated data with articles sorted based on impact score\n",
    "scored_articles = impact2csv(in_data=data)\n",
    "\n",
    "for article in scored_articles[1:]:\n",
    "    title, source, polarity, intensity, damage, impact = article\n",
    "    print(f\"{bcolors.BLUE}Article title: {bcolors.ENDC}\", title)\n",
    "    print(f\"{bcolors.BLUE}Source: {bcolors.ENDC}\", source)\n",
    "    print(f\"{bcolors.RED}Sentiment polarity = {bcolors.ENDC}\", polarity)\n",
    "    print(f\"{bcolors.RED}Emotion intensity = {bcolors.ENDC}\", intensity)\n",
    "    print(f\"{bcolors.RED}Damage-related keywords frequency = {bcolors.ENDC}\", damage)\n",
    "    print(f\"{bcolors.YELLOW}Impact score = {bcolors.ENDC}\", impact)\n",
    "    print(\"________________________________________________________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736be97",
   "metadata": {},
   "source": [
    "### Task 10: Statistical Summary and Visualization Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839e53f",
   "metadata": {},
   "source": [
    "Run `task_10.py` from a terminal:\n",
    "\n",
    "```bash\n",
    "streamlit run task_10.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-prjct-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
