{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf5fa8c",
   "metadata": {},
   "source": [
    "# **Project 17**: Post-Event Damage Assessment Using News Article Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a56715",
   "metadata": {},
   "source": [
    "## Group Members:\n",
    "- Walid Nouicer (wnouicer24@student.oulu.fi)\n",
    "- Uswah Batool (uswah.batool@student.oulu.fi)\n",
    "- Piero Campos Villagaray (pcamposv25@student.oulu.fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda42a1b",
   "metadata": {},
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c1583",
   "metadata": {},
   "source": [
    "### Task 1: Event Selection and Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b912e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_1 import get_urls, get_articles, articles2csv, append_article_to_csv\n",
    "\n",
    "# get_urls() can be used to extract urls from news sources\n",
    "# if no news sources are passed in function parameters, the function uses a default list\n",
    "# this process can take a very long time, for the sake of the demo we will use a preset example\n",
    "# the natural disaster we will be using as an example is Hurricane Melissa, which is an active\n",
    "# Atlantic hurricane currently accelerating northeastward away from the Bahamas and toward Bermuda.\n",
    "example_urls = [\n",
    "    \"https://www.aljazeera.com/gallery/2025/10/30/hurricane-melissa-leaves-trail-of-destruction-across-northern-caribbean\",\n",
    "    \"https://apnews.com/article/hurricane-melissa-jamaica-cuba-landslide-rain-flood-d7d120b8443b1630d12c77e0a3fe25b0\",\n",
    "]\n",
    "\n",
    "# get_articles() filters and scrapes article data from urls based on qwery words passed into the function\n",
    "# we can save articles to a csv file during he function's runtime by setting live_save=True, default value is False\n",
    "# default path for the csv fileis \"data/articles.csv\"\n",
    "qw = [\"hurricane\", \"melissa\"]\n",
    "example_articles = get_articles(example_urls, qw, limit=200, live_save=False)\n",
    "for article in example_articles:\n",
    "    print(f\"\\nTitle: {article[0]}\\ndate: {article[1]}\\nSource: {article[2]}\\nText: {article[3][:200]}...\\n\")\n",
    "    print(\"___________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f17d4",
   "metadata": {},
   "source": [
    "### Task 2: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb619ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_2 import load_file, clean_text\n",
    "\n",
    "# load_file() automatically loads data from \"data/articles.csv\", a different path can be passed in function params\n",
    "# data = load_file()\n",
    "\n",
    "# clean_text() performs lemmatization, and removes stopwords, punctuation, symbols, and converts text to lowecase\n",
    "# cleaned_data = clean_text(data)\n",
    "\n",
    "# articles2csv() can be used to save the processed data\n",
    "# articles2csv(\n",
    "#     articles=cleaned_data,\n",
    "#     path=\"data/cleaned_data.csv\",\n",
    "#     fields=[\"title\", \"date\", \"source\", \"article_text\", \"clean_text\"],\n",
    "# )\n",
    "\n",
    "# the clean_text column is saved as a long string for convenience\n",
    "print(f\"{load_file(path=\"data/cleaned_data.csv\")[1][4][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59a4d6",
   "metadata": {},
   "source": [
    "### Task 3: Language Filtering and Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ab7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from task_3 import filter_english_articles_with_descriptive_stats\n",
    "\n",
    "# filter_english_articles_with_descriptive_stats() filters non-english and duplicate \n",
    "# articles, then prints a quality report\n",
    "\n",
    "data = pd.read_csv(\"data/cleaned_data.csv\")\n",
    "filter_english_articles_with_descriptive_stats(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c842739",
   "metadata": {},
   "source": [
    "### Task 4: Keyword Extraction and Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe24771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_4 import plot_and_save, keyword_analysis\n",
    "\n",
    "# keyword_analysis() extract top keywords using both CountVectorizer and TF-IDF,\n",
    "# visualize results in bar charts and word clouds\n",
    "data = pd.read_csv(\"data/filtered_articles.csv\")\n",
    "keyword_analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699404f",
   "metadata": {},
   "source": [
    "### Task 5: Corpus Statistical Analysis and Zipf’s Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_5 import zipf_analysis\n",
    "\n",
    "# zipf_analysis() performs Zipf’s Law analysis and visualize rank-frequency relationship\n",
    "data = pd.read_csv(\"data/filtered_articles.csv\")\n",
    "zipf_analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671d4e8",
   "metadata": {},
   "source": [
    "### Task 6: Lexical and Readability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_6 import analyze_article_metrics\n",
    "\n",
    "df = pd.read_csv(r\"data/articles.csv\") \n",
    "print(f\"Total articles loaded: {len(df)}\")\n",
    "\n",
    "# analyze_article_metrics() calculates lexical metrics and returns results\n",
    "lexical_results = []\n",
    "for idx, row in df.iterrows():\n",
    "    result = analyze_article_metrics(row)\n",
    "    if result is not None:\n",
    "        lexical_results.append(result)\n",
    "\n",
    "lexical_df = pd.DataFrame(lexical_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LEXICAL METRICS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(lexical_df.describe())  ## Summary Table\n",
    "\n",
    "print(\"\\nSENTENCE COUNT:\")\n",
    "print(f\"  Min sentences: {lexical_df['sentence_count'].min()}\")\n",
    "print(f\"  Max sentences: {lexical_df['sentence_count'].max()}\")\n",
    "print(f\"  Mean sentences: {lexical_df['sentence_count'].mean():.1f}\")\n",
    "\n",
    "lexical_df.to_csv(r\"data/Lexical_Analysis.csv\", index=False)\n",
    "print(f\"\\n Results saved to 'Lexical_Analysis.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e45819",
   "metadata": {},
   "source": [
    "### Task 7: Sentiment and Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb8cb9",
   "metadata": {},
   "source": [
    "### Task 8: Named Entity and Quantitative Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_8 import damage_summary, plot_entity_frequency, aggregate_text\n",
    "\n",
    "# damage_summary() calculate the number of instances in a dataset where and entity\n",
    "# is labeled as MONEY or CARDINAL. for the latter case, filter out\n",
    "# sentences describing distance or speed measurements.\n",
    "\n",
    "aggr_text = aggregate_text(data=load_file()[1:])\n",
    "damage_summary(aggr_text)\n",
    "plot_entity_frequency(aggr_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f3fd9",
   "metadata": {},
   "source": [
    "### Task 9: Event Impact Scoring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_9 import impact2csv\n",
    "\n",
    "data = load_file(\"data/cleaned_data.csv\")\n",
    "\n",
    "# impact2csv() takes csv data and computes sentiment polarity, emotion intensity, damage-related keywords frequency, and impact score\n",
    "# then saves the result into a csv file and returns the generated data with articles sorted based on impact score\n",
    "scored_articles = impact2csv(in_data=data)\n",
    "\n",
    "for article in scored_articles[1:]:\n",
    "    title, source, polarity, intensity, damage, impact = article\n",
    "    print(\"Article aitle: \", title)\n",
    "    print(\"Source: \", source)\n",
    "    print(\"Sentiment polarity = \", polarity)\n",
    "    print(\"Emotion intensity = \", intensity)\n",
    "    print(\"Damage-related keywords frequency = \", damage)\n",
    "    print(\"Impact score = \", impact)\n",
    "    print(\"________________________________________________________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736be97",
   "metadata": {},
   "source": [
    "### Task 10: Statistical Summary and Visualization Dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-prjct-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
